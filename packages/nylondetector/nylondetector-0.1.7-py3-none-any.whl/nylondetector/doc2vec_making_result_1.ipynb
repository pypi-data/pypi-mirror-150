{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining for `D2VTransforemr` Evaluation\n",
    "\n",
    "- Load & Preprocessing -> GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocessing (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "from konlpy.tag import Kkma, Komoran\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "\n",
    "from gensim.sklearn_api import D2VTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nylondetector.preprocess.siu_data_preproc import *\n",
    "from nylondetector.preprocess.text_handling import ma_with_ngrams\n",
    "from document_embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_yyyymm = '2021-09'\n",
    "\n",
    "doubtful_hospital_list = pd.read_csv('hospital_list.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////////////////////////////////////////////\n",
      "////////// Data Loading\n",
      "///////////////////////////////////////////////\n",
      "백내장+부수입: (5, 7)\n",
      "백내장+소개: (1050, 7)\n",
      "백내장+수당: (150, 7)\n",
      "백내장+숙소: (110, 7)\n",
      "백내장+실비: (1050, 7)\n",
      "백내장+실손: (1050, 7)\n",
      "백내장+페이백: (60, 7)\n",
      "백내장+할인: (1050, 7)\n",
      "백내장+호텔: (720, 7)\n",
      "====== successfully done\n",
      "///////////////////////////////////////////////\n",
      "////////// Date column processing\n",
      "///////////////////////////////////////////////\n",
      "====== successfully done\n",
      "///////////////////////////////////////////////\n",
      "////////// Combining to dataframe\n",
      "///////////////////////////////////////////////\n",
      "Raw table---------------------\n",
      "(5245, 7)\n",
      "After filtering---------------\n",
      "(3244, 7)\n",
      "====== successfully done\n",
      "///////////////////////////////////////////////\n",
      "////////// Extracting hashtags\n",
      "///////////////////////////////////////////////\n",
      "====== successfully done\n",
      "///////////////////////////////////////////////\n",
      "////////// Data seperation; by hospital or not\n",
      "///////////////////////////////////////////////\n",
      "Original:  (3244, 8)\n",
      "Blog written by hospital:  (817, 8)\n",
      "Blog written by personal:  (2410, 8)\n",
      "====== successfully done\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing chunk\n",
    "\n",
    "preprocessor = SIUDataPreprocess(target_yyyymm=tgt_yyyymm,\n",
    "                                 data_directory=os.getcwd(),\n",
    "                                 text_columns=['name', 'title', 'content'])\n",
    "\n",
    "preprocessor.load_data_dict()\n",
    "preprocessor.transform_date()\n",
    "preprocessor.combine_data_dict()\n",
    "preprocessor.extract_hashtags(content_col='content', \n",
    "                              hospital_re_target='.*(안과|병원|의원)$')\n",
    "\n",
    "data_hospital, data_prsnl = preprocessor.hospital_prsnl_split(hospital_words_list=['병원', '의원', '외과', '안과'],\n",
    "                                                        non_word_list=['동물', '동물병원'])\n",
    "\n",
    "hospital_names = preprocessor.extract_hospital_name('.*(안과|병원|의원).*').rename('hospital_name')\n",
    "doubtful_hospital_names = preprocessor.match_doubtful_hospitals(hospital_names, \n",
    "                                                             doubtful_hospital_list['병원명'], \n",
    "                                                             0.7).rename('doubtful_hospital_name')\n",
    "\n",
    "data_hospital_2 = pd.concat([data_hospital, hospital_names, doubtful_hospital_names], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 3.38 s, total: 1min 9s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Tagging\n",
    "\n",
    "tagger = Komoran()\n",
    "pos_list = ('NNG', 'NNP', 'NP', 'VV', 'VA')\n",
    "# pos_list = ('NNG', 'NNP', 'NP', 'VV', 'VA', 'XR', 'MAG')\n",
    "max_ngrams = 3\n",
    "\n",
    "data_hospital_ma = ma_with_ngrams(data_hospital_2, 'content', tagger, pos_list, max_ngrams, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keyword Check\n",
    "# target_words_1 = ['실손', '실소', '실비', '수수료', '수당', '할인', '세일', '페이백', '소개',\n",
    "#                   '부수입', '호텔', '숙소']\n",
    "# tagtet_words_2 = ['수_당', '할_인', '실_손', '실_비', '페이_백']\n",
    "\n",
    "# for word in target_words_1:\n",
    "#     data_hospital_ma[word] = data_hospital_ma['content_1gram_2'].map(lambda x: x.count(word))\n",
    "# for word in tagtet_words_2:\n",
    "#     data_hospital_ma[word] = data_hospital_ma['content_2gram'].map(lambda x: x.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블은 추후에 전달받은거라 아래 청크처럼 추가함. \n",
    "- 현재 버전의 전처리와 맞지 않아 코드 검증용으로만 활용, 유효성 낮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('siu_백내장_label_ver1.txt', header=None)[:816]\n",
    "\n",
    "data_hospital_ma['label'] = labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "- 현재는 `D2VTransformer`의 분류 시 성능을 체크하는 게 목적  \n",
    "- `FeatureUnion` 활용해 앞 전처리부분은 추가 및 변경 가능\n",
    "- Keyword generator 등의 custom transformer를 class로 구축해 활용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    499\n",
       "1.0    108\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hospital_ma = data_hospital_ma.loc[data_hospital_ma['content'].map(lambda x: '동물병원' not in x)]\n",
    "data_hospital_ma = data_hospital_ma.loc[data_hospital_ma['label'].map(lambda x: x!=2)]\n",
    "data_hospital_ma['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding = DocumentEmbedding(ngram_names=['content_1gram_1', 'content_2gram', 'content_3gram'],\n",
    "                                  ngram_ns=[1, 2, 3],\n",
    "                                  how_ngram='1_2_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filtering monograms only\n",
    "# [x for x in X[0] if '_' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc_embedding.make_input(data_hospital_ma)\n",
    "y = data_hospital_ma['label'].reset_index(drop=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                        ('embed', D2VTransformer()),\n",
    "                        ('clf', xgb.XGBClassifier())\n",
    "                    ]) #TFIDFTransformer 추가예정 via FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('embed',\n",
       "   D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1, comment=None,\n",
       "                  dbow_words=0, dm=1, dm_concat=0, dm_mean=None, dm_tag_count=1,\n",
       "                  docvecs=None, docvecs_mapfile=None,\n",
       "                  hashfxn=<built-in function hash>, hs=0, iter=5,\n",
       "                  max_vocab_size=None, min_alpha=0.0001, min_count=5, negative=5,\n",
       "                  sample=0.001, seed=1, size=100, sorted_vocab=1, trim_rule=None,\n",
       "                  window=5, workers=3)),\n",
       "  ('clf',\n",
       "   XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "                 gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "                 learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                 objective='binary:logistic', random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None))],\n",
       " 'verbose': False,\n",
       " 'embed': D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1, comment=None,\n",
       "                dbow_words=0, dm=1, dm_concat=0, dm_mean=None, dm_tag_count=1,\n",
       "                docvecs=None, docvecs_mapfile=None,\n",
       "                hashfxn=<built-in function hash>, hs=0, iter=5,\n",
       "                max_vocab_size=None, min_alpha=0.0001, min_count=5, negative=5,\n",
       "                sample=0.001, seed=1, size=100, sorted_vocab=1, trim_rule=None,\n",
       "                window=5, workers=3),\n",
       " 'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "               colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "               gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "               objective='binary:logistic', random_state=None, reg_alpha=None,\n",
       "               reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "               tree_method=None, validate_parameters=None, verbosity=None),\n",
       " 'embed__alpha': 0.025,\n",
       " 'embed__batch_words': 10000,\n",
       " 'embed__cbow_mean': 1,\n",
       " 'embed__comment': None,\n",
       " 'embed__dbow_words': 0,\n",
       " 'embed__dm': 1,\n",
       " 'embed__dm_concat': 0,\n",
       " 'embed__dm_mean': None,\n",
       " 'embed__dm_tag_count': 1,\n",
       " 'embed__docvecs': None,\n",
       " 'embed__docvecs_mapfile': None,\n",
       " 'embed__hashfxn': <function hash(obj, /)>,\n",
       " 'embed__hs': 0,\n",
       " 'embed__iter': 5,\n",
       " 'embed__max_vocab_size': None,\n",
       " 'embed__min_alpha': 0.0001,\n",
       " 'embed__min_count': 5,\n",
       " 'embed__negative': 5,\n",
       " 'embed__sample': 0.001,\n",
       " 'embed__seed': 1,\n",
       " 'embed__size': 100,\n",
       " 'embed__sorted_vocab': 1,\n",
       " 'embed__trim_rule': None,\n",
       " 'embed__window': 5,\n",
       " 'embed__workers': 3,\n",
       " 'clf__objective': 'binary:logistic',\n",
       " 'clf__base_score': None,\n",
       " 'clf__booster': None,\n",
       " 'clf__colsample_bylevel': None,\n",
       " 'clf__colsample_bynode': None,\n",
       " 'clf__colsample_bytree': None,\n",
       " 'clf__gamma': None,\n",
       " 'clf__gpu_id': None,\n",
       " 'clf__importance_type': 'gain',\n",
       " 'clf__interaction_constraints': None,\n",
       " 'clf__learning_rate': None,\n",
       " 'clf__max_delta_step': None,\n",
       " 'clf__max_depth': None,\n",
       " 'clf__min_child_weight': None,\n",
       " 'clf__missing': nan,\n",
       " 'clf__monotone_constraints': None,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__num_parallel_tree': None,\n",
       " 'clf__random_state': None,\n",
       " 'clf__reg_alpha': None,\n",
       " 'clf__reg_lambda': None,\n",
       " 'clf__scale_pos_weight': None,\n",
       " 'clf__subsample': None,\n",
       " 'clf__tree_method': None,\n",
       " 'clf__validate_parameters': None,\n",
       " 'clf__verbosity': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Labels: [0. 1.]\n",
      "\n",
      "-------Confusion Matrix\n",
      "[[118  12]\n",
      " [ 17   5]]\n",
      "\n",
      "---Accuary: 0.8092105263157895\n",
      "---F1 Score: 0.25641025641025644\n",
      "-------Best Parameters\n",
      "{'embed__window': 7}\n",
      "CPU times: user 2min 22s, sys: 1.58 s, total: 2min 23s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parameters_dict = {\n",
    "    'embed__window' : [5,6,7]\n",
    "}\n",
    "\n",
    "best_params_1 = doc_embedding.doc2vec_cv(X, y, pipeline, parameters_dict, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결정된 매개변수 적용해 Embedding 진행 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_2 = dict()\n",
    "for key in best_params_1.keys():\n",
    "    best_params_2[key.split('__')[1]] = best_params_1[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1, comment=None,\n",
       "               dbow_words=0, dm=1, dm_concat=0, dm_mean=None, dm_tag_count=1,\n",
       "               docvecs=None, docvecs_mapfile=None,\n",
       "               hashfxn=<built-in function hash>, hs=0, iter=5,\n",
       "               max_vocab_size=None, min_alpha=0.0001, min_count=5, negative=5,\n",
       "               sample=0.001, seed=1, size=100, sorted_vocab=1, trim_rule=None,\n",
       "               window=5, workers=3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_transformer = D2VTransformer()\n",
    "d2v_transformer.set_params(**best_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-eca0a929336d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdoc_embedded_vector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0md2v_transformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mpickle_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'doc_embedded_vector.pkl'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdoc_embedded_vector\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/inbaebae.kang2/10. SIU/pkl_save_load.py\u001B[0m in \u001B[0;36mpickle_save\u001B[0;34m(filename, object_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mpickle_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobject_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m         \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobject_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "def pickle_save(filename, object_name):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(object_name, f)\n",
    "\n",
    "doc_embedded_vector = d2v_transformer.fit_transform(X)\n",
    "pickle_save('doc_embedded_vector.pkl', doc_embedded_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}