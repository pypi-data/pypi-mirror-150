# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/13_text-modeling-token-classification.ipynb (unless otherwise specified).

__all__ = ['calculate_token_class_metrics', 'TokenClassMetricsCallback', 'TokenAggregationStrategies',
           'BlearnerForTokenClassification']

# Cell
import os, ast, inspect
from typing import Any, Callable, Dict, List, Optional, Union, Type

from fastcore.all import *
from fastai.callback.all import *
from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter
from fastai.data.core import DataLoaders
from fastai.imports import *
from fastai.learner import *
from fastai.optimizer import Adam, params
from fastai.metrics import perplexity
from fastai.torch_core import *
from fastai.torch_imports import *
from seqeval import metrics as seq_metrics
from transformers import AutoModelForTokenClassification, PreTrainedTokenizerBase, PreTrainedModel, logging

from ..data.core import TextBlock, TextDataLoader, get_blurr_tfm, first_blurr_tfm
from ..data.token_classification import (
    get_token_labels_from_input_ids,
    get_word_labels_from_token_labels,
    TokenClassTextInput,
    TokenTensorCategory,
    TokenCategorize,
    TokenCategoryBlock,
    TokenClassBatchTokenizeTransform,
)
from .core import Blearner
from ..utils import get_hf_objects
from ...utils import PreCalculatedCrossEntropyLoss

logging.set_verbosity_error()


# Cell
def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):
    if metric_key == "accuracy":
        return seq_metrics.accuracy_score(targ_toks, pred_toks)

    if metric_key == "precision":
        return seq_metrics.precision_score(targ_toks, pred_toks)

    if metric_key == "recall":
        return seq_metrics.recall_score(targ_toks, pred_toks)

    if metric_key == "f1":
        return seq_metrics.f1_score(targ_toks, pred_toks)

    if metric_key == "classification_report":
        return seq_metrics.classification_report(targ_toks, pred_toks)


# Cell
class TokenClassMetricsCallback(Callback):
    """
    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the
    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's
    calculations.

    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.
    """

    def __init__(self, tok_metrics=["accuracy", "precision", "recall", "f1"], **kwargs):
        self.run_before = Recorder

        store_attr(self=self, names="tok_metrics, kwargs")
        self.custom_metrics_dict = {k: None for k in tok_metrics}

        self.do_setup = True

    def setup(self):
        # one time setup code here.
        if not self.do_setup:
            return

        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform
        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])
        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)

        self.hf_tokenizer = tfm.hf_tokenizer
        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id
        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())
        self.tok_kwargs = tfm.kwargs

        # add custom text generation specific metrics
        custom_metric_keys = self.custom_metrics_dict.keys()
        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])
        self.learn.metrics = self.learn.metrics + custom_metrics
        self.learn.token_classification_report = None

        self.do_setup = False

    def before_fit(self):
        self.setup()

    # --- batch begin/after phases ---
    def before_batch(self):
        pass

    def after_batch(self):
        if self.training or self.learn.y is None:
            return

        # do this only for validation set
        preds = self.pred.argmax(dim=-1)
        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data

        preds_list, targets_list = [], []
        for i in range(targs.shape[0]):
            item_targs, item_preds = [], []

            for j in range(targs.shape[1]):
                if targs[i, j] != self.ignore_label_token_id:
                    item_preds.append(self.dls.vocab[preds[i][j].item()])
                    item_targs.append(self.dls.vocab[targs[i][j].item()])

            preds_list.append(item_preds)
            targets_list.append(item_targs)

        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]

    # --- validation begin/after phases ---
    def before_validate(self):
        self.results = []

    def after_validate(self):
        if len(self.results) < 1:
            return

        preds, targs = map(list, zip(*self.results))
        for k in self.custom_metrics_dict.keys():
            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)

        try:
            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, "classification_report")
        except ZeroDivisionError as err:
            print(f"Couldn't calcualte classification report: {err}")

    # --- for ValueMetric metrics ---
    def metric_value(self, metric_key):
        return self.custom_metrics_dict[metric_key]


# Cell
@typedispatch
def show_results(
    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs
    x: TokenClassTextInput,
    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets
    y: TokenTensorCategory,
    # Your raw inputs/targets
    samples,
    # The model's predictions
    outs,
    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into
    # something understandable
    learner,
    # Your `show_results` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
    # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_results`
    **kwargs,
):
    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])
    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer
    ignore_token_id = tfm.ignore_token_id
    vocab = learner.dls.vocab

    res = L()
    for inp, trg, sample, pred in zip(x, y, samples, outs):
        # align "tokens" with labels
        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)
        # align "words" with labels
        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)
        # align "words" with "predicted" labels
        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]
        # stringify list of (word,label) for example
        res.append(
            [
                f"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}"
            ]
        )

    display_df(pd.DataFrame(res, columns=["token / target label / predicted label"])[:max_n])
    return ctxs


# Cell
class TokenAggregationStrategies:
    """
    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various
    token classication tasks (e.g, NER, POS, chunking, etc...)
    """

    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = "O") -> None:
        self.hf_tokenizer = hf_tokenizer
        self.labels = labels
        self.non_entity_label = non_entity_label
        self.valid_strategies = ["simple", "first", "max", "average"]

        self.uses_BI_label_strategy = False
        for lbl in self.labels:
            if lbl.startswith("I-"):
                self.uses_BI_label_strategy = True
                break

    def by_token(self, tokens, input_ids, offsets, preds, probs):
        results = []
        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):
            # pass over any non-entity labels and "special" tokens
            label = self.labels[pred]
            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:
                continue

            start, end = offset
            results.append({"entity": label, "score": prob[pred], "word": token, "start": start.item(), "end": end.item()})

        return results

    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):
        # validate `strategy_name`
        if strategy_name not in self.valid_strategies:
            raise ValueError("The 'strategy_name' is not supported by this class")

        # validate the existence of `word_ids` if the aggregation strategy = "average"
        if strategy_name == "average" and word_ids is None:
            raise ValueError("The 'average' strategy requires word_ids list")

        results = []
        idx = 0
        while idx < len(preds):
            pred = preds[idx]
            label = self.labels[pred]

            # pass over any non-entity labels and "special" tokens
            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:
                idx += 1
                continue

            # Remove the B- or I-
            label = label[2:] if self.uses_BI_label_strategy else label
            start, end = offsets[idx]

            all_scores = []
            all_scores.append(probs[idx][pred])

            word_scores = {}
            if strategy_name == "average":
                word_scores[word_ids[idx]] = [probs[idx][pred]]

            lbl_to_search = f"I-{label}" if self.uses_BI_label_strategy else label
            while idx + 1 < len(preds) and self.labels[preds[idx + 1]] == lbl_to_search:
                idx += 1
                _, end = offsets[idx]

                pred = preds[idx]

                if strategy_name == "average":
                    if word_ids[idx] in word_scores:
                        word_scores[word_ids[idx]].append(probs[idx][pred])
                    else:
                        word_scores[word_ids[idx]] = [probs[idx][pred]]

                if strategy_name != "first":
                    all_scores.append(probs[idx][pred])

            # The score is the mean of all the scores of the tokens in that grouped entity
            if strategy_name == "average":
                score = np.mean([np.mean(v).item() for k, v in word_scores.items()])
            else:
                score = np.max(all_scores).item() if strategy_name == "max" else np.mean(all_scores).item()

            word = text[start:end]
            results.append({"entity_group": label, "score": score, "word": word, "start": start.item(), "end": end.item()})

            idx += 1

        return results


# Cell
@patch
def blurr_predict_tokens(
    self: Learner,
    # The str (or list of strings) you want to get token classification predictions for
    items: Union[str, List[str]],
    # How entities are grouped and scored
    aggregation_strategy: str = "simple",
    # The label used to idendity non-entity related words/tokens
    non_entity_label: str = "O",
    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a
    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the
    # equavlient of fast tokenizer's `word_ids``
    slow_word_ids_func: Optional[Callable] = None,
):
    if not is_listy(items):
        items = [items]

    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])
    hf_tokenizer = tfm.hf_tokenizer

    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)

    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors="pt", padding=True, truncation=True)
    inputs_offsets = inputs["offset_mapping"]
    inputs_input_ids = inputs["input_ids"]

    # run inputs through model
    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}
    outputs = self.model(model_inputs)

    # fetch probabilities and predictions
    probabilities = F.softmax(outputs.logits, dim=-1).tolist()
    predictions = outputs.logits.argmax(dim=-1).tolist()

    # build our results
    results = []
    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(
        zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)
    ):
        # build our results for the current input
        tokens = inputs.tokens(input_idx)
        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)

        if aggregation_strategy == "token":
            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))
        else:
            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))
    return results


# Cell
@delegates(Blearner.__init__)
class BlearnerForTokenClassification(Blearner):
    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):
        super().__init__(dls, hf_model, **kwargs)

    def predict(self, text):
        return self.blurr_predict_tokens(text)

    @classmethod
    def get_model_cls(self):
        return AutoModelForTokenClassification

    @classmethod
    def get_metrics_cb(self):
        return TokenClassMetricsCallback()

    @classmethod
    def from_data(
        cls,
        # Your raw dataset. Supports DataFrames, Hugging Face Datasets, as well as file paths
        # to .csv, .xlsx, .xls, and .jsonl files
        data: Union[pd.DataFrame, Path, str, List[Dict]],
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        # The attribute in your dataset that contains a list of your tokens
        tokens_attr: List[str] = "tokens",
        # The attribute in your dataset that contains the entity labels for each token in your raw text
        token_labels_attr: List[str] = "token_labels",
        # The unique entity labels (or vocab) available in your dataset
        labels: Optional[List[str]] = None,
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter: Optional[Callable] = None,
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs: dict = {},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs: dict = {},
    ):
        # if we get a path/str then we're loading something like a .csv file
        if isinstance(data, Path) or isinstance(data, str):
            content_type = mimetypes.guess_type(data)[0]
            if content_type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
                data = pd.read_excel(data)
            elif content_type == "text/csv":
                data = pd.read_csv(data)
            elif content_type == "application/json":
                data = pd.read_json(data, orient="records")
            else:
                raise ValueError("'data' must be a .xlsx, .xls, .csv, or .jsonl file")

            data = pd.read_csv(data)

        # we need to tell transformer how many labels/classes to expect
        if labels is None:
            if isinstance(data, pd.DataFrame):
                labels = sorted(list(set([lbls for sublist in data[token_labels_attr].tolist() for lbls in sublist])))
            else:
                labels = sorted(list(set([item[token_labels_attr] for item in data])))

        # infer our datablock splitter if None
        if dblock_splitter is None:
            dblock_splitter = ColSplitter() if hasattr(data, "is_valid") else RandomSplitter()

        # get our hf objects
        n_labels = len(labels)
        hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(
            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={"num_labels": n_labels}
        )

        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here
        if hf_tokenizer.pad_token is None:
            hf_tokenizer.add_special_tokens({"pad_token": "<pad>"})
            hf_config.pad_token_id = hf_tokenizer.get_vocab()["<pad>"]
            hf_model.resize_token_embeddings(len(hf_tokenizer))

        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)
        blocks = (
            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),
            TokenCategoryBlock(vocab=labels),
        )

        dblock = DataBlock(blocks=blocks, get_x=ItemGetter(tokens_attr), get_y=ItemGetter(token_labels_attr), splitter=dblock_splitter)
        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance
        return cls(dls, hf_model, **learner_kwargs.copy())
